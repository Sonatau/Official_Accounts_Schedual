>  **译者：张峰，Datawhale成员**

![img](https://mmbiz.qpic.cn/mmbiz_jpg/vI9nYe94fsGnicI5IibBvGy8iau9NDVXDHao5snX08PhMAnfJXuUsPTuyO4Gib8CdZbPNfx1RPcFRlt8VSkwDuR3Jg/640?wx_fmt=jpeg)

如果你最近刚入手Kaggle，或者你是这个平台的老常客，你可能会想知道如何轻松提升模型的性能。以下是我在Kaggle历程中积累的一些实用技巧（https://www.kaggle.com/louise2001）。所以，无论是建立自己的模型，还是从一个基准的公共内核开始，都可以尝试实施这些建议！

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsGnicI5IibBvGy8iau9NDVXDHaFQiboVYvEf8wCLNibcfh05EPWLBFf6kCtobrcyyPBGqYtqQcDMnse6Gw/640?wx_fmt=png)

## **一、经常回顾过去的比赛（温故知新）** 

虽然Kaggle的策略是绝对不会出现两次一模一样的比赛，但经常会出现非常相似的问题的翻版。例如，有些主办方每年都会提出同一个主题的常规挑战（比如NFL的Big Data Bowl），只是有一些小的变化，或者在某些领域（比如医学影像）有很多目标不同但本质非常相似的比赛。

因此，回顾获奖者的解决方案（总是在竞争结束后公开，这要感谢不可思议的Kaggle社区）可以是一个很好的加分项，因为它给你提供了开始的想法，以及一个成功的策略。如果你有时间回顾很多，你也会很快发现，即使在非常不同的比赛中，一些流行的基准模型似乎总是做得足够好：

- 卷积神经网络或计算机视觉挑战中更复杂的ResNet或EfficientNet。
- WaveNet在音频处理中的挑战（如果你只是使用Mel Spectrogram，也可以很好地用图像识别模型来处理）。
- BERT及其衍生产品（RoBERTa等）在自然语言处理中的挑战。
- Light Gradient Boosting（或其他梯度提升或树策略）对表格数据的处理。

你可以直接在Kaggle平台上寻找类似的比赛，或者查看Sundalai Rajkumar的这篇精彩总结（https://www.kaggle.com/sudalairajkumar/winning-solutions-of-kaggle-competitions）。

回顾过去的比赛也可以帮助你获得其他步骤的提示。例如，获得类似问题的预处理的提示和技巧，人们如何选择他们的超参数，在他们的模型中实现了什么额外的工具来赢得比赛，或者如果他们只专注于囊括他们最好的模型的相似版本，或者是将所有可用公共内核整合在一起。

## **二、花足够的时间在数据预处理上**

这绝不是工作中最令人兴奋的部分。但是，这一步骤的重要性不言而喻：

- **清理数据**：千万不要以为主机努力为你提供最干净的数据。大多数时候，它是错误的。填充NaNs，去除离群值，将数据分成同质观测值的类别等。
- 做一些简单的**探索性数据分析**，以大致了解你正在使用的工具（这将有助于您获得见解和想法）。
- **增强数据**：这可能是提高性能的最好方法之一。但是要注意不要让数据太大，以至于你的模型无法再处理它。你可以在互联网上找到一些额外的数据集，或者在Kaggle平台上（在过去类似的比赛中！），或者只是在提供给你的数据上工作：翻转和裁剪图像，叠加音频记录，反向翻译或替换文本中的同义词等。

预处理也是你必须思考你将依靠什么交叉验证方法的步骤：Trust Your CV。在你的数据上工作将帮助你知道如何分割数据：根据目标值分层还是根据样本类别分层 ? 你的数据是否不平衡 ? 如果你有一个聪明的CV策略，并且仅依靠它而不是靠排行榜得分（尽管这可能很诱人），那么你很有可能在私人的最终分数上获得惊喜。

## **三、尝试超参数搜索**

超参数搜索可以帮助你找到你的模型应该具备的最佳参数（学习率、softmax的温度等），以获得最佳的性能，而无需手工运行上千次的枯燥实验。

最常见的超参数搜索策略包括：

- **网格搜索**(请永远不要这样做)：对我来说是性能最差的方法，因为对于某些值，你可能会完全错过某个模式或性能的局部峰值，它包括或测试超参数值平均分布在你定义的可能值的区间上。
- **随机搜索**（及其Monte-Carlo衍生物）：尝试参数的随机值。它的主要问题在于它是一种并行的方法，而且你测试的参数越多，成本就越高。然而，它的优点是可以让你在测试中加入先验知识：如果你想找到1e-4和1e-1之间的最佳学习率，但你认为它必须在1e-3左右，你可以从以1e-3为中心的对数正态分布中抽取样本。
- **贝叶斯搜索**：基本上是随机搜索，但经过改进，因为它是迭代的，因此成本低得多。它根据当前模型迭代评估一个有希望的超参数配置，然后更新它。它是这三种方法中性能最好的一种。
- 其他方法包括基于梯度的搜索或演化优化，危险性较大，一般不适用。在一些特殊情况下，可以推荐使用这些方法。

## **四、简单的包装器可以改变游戏规则**

我发现有一些模型包装器可以用来获得更好的结果。它们在不同级别上工作：

- 在优化过程中，千万不要忘了添加一个学习率调度器，帮助获得更精确的训练（从小开始，当你的模型学习良好时逐步增加）。
- 还是在优化过程中，你可以把Lookahead包在你的优化器上；Lookahead算法包括向前走k个优化步骤，找到性能最好的地方，然后向最佳方向后退一步，从那里重新开始训练。理论上，你可以获得更好的性能，虽然我从来没有发现这是真的；但它可以稳定训练，当你的数据非常嘈杂时，这是好事。
- 在开始训练之前，为你的权重找到一个好的初始化：如果你使用的是流行的架构，就从基准权重开始（比如图像识别中的ImageNet），如果不是，可以尝试Layer Sequential Unit Variance初始化（LSUV，理论上是最好的初始化）。它包括将你的权重初始化为正交的，并且在所有可训练层中都是单位方差。
- 最后，我经常发现，从神经网络的最后一层权重来训练LGBM，而不是添加一个softmax作为输出层，效果会出奇的好。

## **五、Bagging集成在一起！**

除了数据增强，可能没有什么技术比bagging更有效地提高你的性能。

我个人的小技巧是：总是保存我运行过的每一个模型预测，并对所有模型进行平均（只是基本的平均，我从来没有发现任何证据表明，"聪明 "的集成，如按模型的单一性能加权，在最终得分中增加了什么）。

不要忘记把公共内核也进行装袋。你的集成策略中的模型越多，你就越有可能在私人排行榜中稳操胜券。

原文链接：https://towardsdatascience.com/5-simple-tips-to-improve-your-kaggle-models-159c00523418

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsGxu3P5YibTO899okS0X9WaLmQCtia4U8Eu1xWCz9t8Qtq9PH6T1bTcxibiaCIkGzAxpeRkRFYqibVmwSw/640?wx_fmt=png)

